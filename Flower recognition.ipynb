{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                  基于神经网络的花朵分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、搭建CNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1：获得图片数据\n",
    "\n",
    "解压数据集到./datasets/flowers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daisy count:633\n",
      "dandelion count:898\n",
      "sunflowers count:699\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "os.chdir('C:/Users/zhoumeng/Anaconda3/envs/tensorFlow/Lib/site-packages/keras') ##定位\n",
    "daisy1 ='./datasets/flowers/daisy/'##数据所在目录\n",
    "dandelion1='./datasets/flowers/dandelion/'\n",
    "sunflowers1='./datasets/flowers/sunflowers/'\n",
    "daisy = [daisy1 + i for i in os.listdir(daisy1) ]\n",
    "dandelion = [dandelion1 + i for i in os.listdir(dandelion1) ]\n",
    "sunflowers = [sunflowers1 + i for i in os.listdir(sunflowers1) ]\n",
    "##查看每个里面有多少数据\n",
    "print('daisy count:' + str(len(daisy)))\n",
    "print('dandelion count:' + str(len(dandelion)))\n",
    "print('sunflowers count:' + str(len(sunflowers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/flowers/daisy/2019064575_7656b9340f_m.jpg\n"
     ]
    }
   ],
   "source": [
    "print(daisy[1])##查看一下是否传输正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2：拆分训练集和验证集\n",
    "注：这里为了加快速度，只选择了600张图片作为训练集，150张图片作为测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = './datasets/flowers/arrange/' # 目标训练集地址\n",
    "# 随机化\n",
    "random.shuffle(daisy)\n",
    "random.shuffle(dandelion)\n",
    "random.shuffle(sunflowers)\n",
    "def ensure_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        try:\n",
    "            os.makedirs(dir_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "# 生成文件夹\n",
    "ensure_dir(target + 'train/daisy')\n",
    "ensure_dir(target + 'train/dandelion')\n",
    "ensure_dir(target + 'train/sunflowers')\n",
    "ensure_dir(target + 'validation/daisy')\n",
    "ensure_dir(target + 'validation/dandelion')\n",
    "ensure_dir(target + 'validation/sunflowers')\n",
    "# 复制图片\n",
    "##选每类图片的前200个图作为训练集共600个\n",
    "for daisy_file,  dandelion_file, sunflowers_file in list(zip(daisy, dandelion,sunflowers))[:200]:\n",
    "    shutil.copyfile(daisy_file, target + 'train/daisy/' + os.path.basename(daisy_file))\n",
    "    shutil.copyfile(dandelion_file, target + 'train/dandelion/' + os.path.basename(dandelion_file))\n",
    "    shutil.copyfile(sunflowers_file, target + 'train/sunflowers/' + os.path.basename(sunflowers_file))\n",
    "\n",
    "##选每类图片的200-250个图作为测试集共150个\n",
    "for daisy_file,  dandelion_file, sunflowers_file in list(zip(daisy, dandelion,sunflowers))[200:250]:\n",
    "    shutil.copyfile(daisy_file, target + 'validation/daisy/' + os.path.basename(daisy_file))\n",
    "    shutil.copyfile(dandelion_file, target + 'validation/dandelion/' + os.path.basename(dandelion_file))\n",
    "    shutil.copyfile(sunflowers_file, target + 'validation/sunflowers/' + os.path.basename(sunflowers_file))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3：处理图片数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhoumeng\\Anaconda3\\envs\\tensorFlow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# 图片尺寸\n",
    "img_width, img_height = 128, 128\n",
    "input_shape = (img_width, img_height, 3)\n",
    "train_data_dir = target + 'train'\n",
    "validation_data_dir = target + 'validation'\n",
    "\n",
    "# 生成变形图片\n",
    "train_pic_gen = ImageDataGenerator(\n",
    "        rescale=1./255, # 对输入图片归一化到0-1区间\n",
    "        rotation_range=20, \n",
    "        width_shift_range=0.2, \n",
    "        height_shift_range=0.2, \n",
    "        shear_range=0.2, \n",
    "        zoom_range=0.5, \n",
    "        horizontal_flip=True, # 水平翻转\n",
    "        fill_mode='nearest')\n",
    "# 测试集不做变形处理，只需要归一化。\n",
    "validation_pic_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# 按文件夹生成训练集流和标签 categorical三分类问题\n",
    "##batch_size: batch数据的大小,默认32\n",
    "train_flow = train_pic_gen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "# 按文件夹生成测试集流和标签 categorical\n",
    "validation_flow = validation_pic_gen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4:搭建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "steps_per_epoch=2000\n",
    "##整数，当生成器返回steps_per_epoch次数据时计一个epoch结束，执行下一个epoch\n",
    "validation_steps = 800\n",
    "##官方说明文档中例子给出的数字2000和800\n",
    "##当validation_data为生成器时，本参数指定验证集的生成器返回次数\n",
    "epochs = 1\n",
    "# 两层卷积-池化，提取64个平面特征\n",
    "model = Sequential([\n",
    "Convolution2D(32, (3, 3), input_shape=input_shape, activation='relu'),\n",
    "MaxPooling2D(pool_size=(2, 2)),\n",
    "Convolution2D(64, (3, 3), activation='relu'),\n",
    "MaxPooling2D(pool_size=(2, 2)),\n",
    "Flatten(),\n",
    "Dense(64, activation='relu'),\n",
    "Dropout(0.5),\n",
    "Dense(3, activation='softmax'),\n",
    "])\n",
    "##drop设置为0.5\n",
    "# 损失函数\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "##优化器设置为RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：这里由于epoch设置为1，所以为了准确率降steps_per_epoch设置为2000，validation_steps设置为800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5：利用搭建好的神经网络进行识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 2052s 1s/step - loss: 0.5354 - acc: 0.7898 - val_loss: 0.4616 - val_acc: 0.8398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x199e3149ac8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_flow,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_flow,\n",
    "        validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "    在这一部分中，利用Sequential的方法搭建了层数为2的神经网络模型，在最后的结果中，训练集的准确度达到了0.79，测试集的准确度达到了0.84，效果是不错的，对比老师展示的“猫狗识别问题”，这里虽然训练集和测试集都比老师少，但是因为由于是一个三分类问题，在参数相同的情况下，因此时间还是要长一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、迁移学习--微训练模型（fine-tuning）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迁移VGG16模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# 图片尺寸\n",
    "img_width, img_height = 128, 128\n",
    "input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用Model的方法搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Flatten, Dense\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "y = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理图片数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注:为了加快速度，这里的epochs设置为1，steps_per_epoch设置为1000，validation_steps设置为800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# 数据地址\n",
    "target = './datasets/flowers/arrange/' # 目标训练集地址\n",
    "train_data_dir = target + 'train'\n",
    "validation_data_dir = target + 'validation'\n",
    "# 模型参数\n",
    "steps_per_epoch = 1000\n",
    "validation_steps = 800\n",
    "epochs = 1\n",
    "# 生成变形图片，并做去均值处理，已做好的可以省略这步，延续之前生存的变形图片\n",
    "train_pic_gen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "# 测试集不做变形处理，只需要去均值\n",
    "validation_pic_gen = ImageDataGenerator(rescale=1./255)\n",
    "# 按文件夹生成训练集流和标签\n",
    "train_flow = train_pic_gen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "# 按文件夹生成测试集流和标签\n",
    "validation_flow = validation_pic_gen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 冻结VGG中ImageNet的CNN结构部分，让ImageNet训练好的参数不变\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用搭建好的模型进行图片识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 10431s 10s/step - loss: 0.2633 - acc: 0.9068 - val_loss: 0.5447 - val_acc: 0.8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x199888292e8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(\n",
    "        train_flow,\n",
    "        steps_per_epoch=steps_per_epoch,#1000\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_flow,#800\n",
    "        validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 小结：\n",
    "    在这一部分中，迁移已有的vgg16，利用model的方法搭建神经网络，最后结果训练集的准确率为0.91，测试集的准确率为0.88，相较于上一部分自己搭建的2层的神经网络，在迭代步长（steps_per_epoch）少了一半的情况下，准确率依旧有所提高，效果较好，这一部分跑了将近3个小时。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
